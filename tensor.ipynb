{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cde91222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Iterable, List, Sequence, Tuple, Union, Optional, Literal\n",
    "import math\n",
    "import random\n",
    "\n",
    "Number = Union[int, float]\n",
    "\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Sequence[Number],\n",
    "        shape: Sequence[int] = (),\n",
    "        *,\n",
    "        requires_grad: bool = False,\n",
    "        _children: Sequence[\"Tensor\"] = (),\n",
    "        _op: str = \"\",\n",
    "    ):\n",
    "        self.shape = tuple(int(s) for s in shape)\n",
    "        self.size = math.prod(self.shape) if self.shape else 1\n",
    "        flat = list(data)\n",
    "        if len(flat) != self.size:\n",
    "            raise ValueError(f\"Data size {len(flat)} != shape size {self.size}\")\n",
    "        self.data: List[float] = [float(x) for x in flat]  # row-major flat storage\n",
    "        self.strides = self._compute_strides(self.shape)\n",
    "        self.requires_grad = bool(requires_grad)\n",
    "        self.grad: Optional[List[float]] = [0.0] * self.size if self.requires_grad else None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self._backward = lambda: None\n",
    "        self._name: Optional[str] = None\n",
    "\n",
    "    def named(self, name: str) -> \"Tensor\":\n",
    "        self._name = name\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_strides(shape: Sequence[int]) -> Tuple[int, ...]:\n",
    "        if not shape:\n",
    "            return ()\n",
    "        strides = [1] * len(shape)\n",
    "        for i in range(len(shape) - 2, -1, -1):\n",
    "            strides[i] = strides[i + 1] * shape[i + 1]\n",
    "        return tuple(strides)\n",
    "\n",
    "    @staticmethod\n",
    "    def zeros(shape: Sequence[int], *, requires_grad: bool = False) -> \"Tensor\":\n",
    "        size = math.prod(shape) if shape else 1\n",
    "        return Tensor([0.0] * size, shape, requires_grad=requires_grad)\n",
    "\n",
    "    @staticmethod\n",
    "    def ones(shape: Sequence[int], *, requires_grad: bool = False) -> \"Tensor\":\n",
    "        size = math.prod(shape) if shape else 1\n",
    "        return Tensor([1.0] * size, shape, requires_grad=requires_grad)\n",
    "\n",
    "    @staticmethod\n",
    "    def randn(\n",
    "        shape: Sequence[int],\n",
    "        *,\n",
    "        mean: float = 0.0,\n",
    "        std: float = 1.0,\n",
    "        requires_grad: bool = False,\n",
    "        seed: Optional[int] = None,\n",
    "    ) -> \"Tensor\":\n",
    "        if seed is not None:\n",
    "            rnd = random.Random(seed)\n",
    "            data = [rnd.gauss(mean, std) for _ in range(math.prod(shape) if shape else 1)]\n",
    "        else:\n",
    "            data = [random.gauss(mean, std) for _ in range(math.prod(shape) if shape else 1)]\n",
    "        return Tensor(data, shape, requires_grad=requires_grad)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_nested(nested: Sequence) -> \"Tensor\":\n",
    "        shape: List[int] = []\n",
    "        temp = nested\n",
    "        while isinstance(temp, list):\n",
    "            shape.append(len(temp))\n",
    "            temp = temp[0] if temp else []\n",
    "        flat: List[Number] = []\n",
    "\n",
    "        def _flatten(x):\n",
    "            if isinstance(x, list):\n",
    "                for v in x:\n",
    "                    _flatten(v)\n",
    "            else:\n",
    "                flat.append(x)\n",
    "\n",
    "        _flatten(nested)\n",
    "        return Tensor(flat, shape)\n",
    "\n",
    "    def to_nested(self) -> List:\n",
    "        def _build(shape, offset):\n",
    "            if not shape:\n",
    "                return self.data[offset]\n",
    "            step = self._compute_strides(shape)[0]\n",
    "            return [_build(shape[1:], offset + i * step) for i in range(shape[0])]\n",
    "\n",
    "        return _build(list(self.shape), 0)\n",
    "\n",
    "    def item(self) -> float:\n",
    "        if self.size != 1:\n",
    "            raise ValueError(\"item() only valid for size==1\")\n",
    "        return float(self.data[0])\n",
    "\n",
    "    def _index(self, idx: Sequence[int]) -> int:\n",
    "        if len(idx) != len(self.shape):\n",
    "            raise IndexError(\"Incorrect number of indices\")\n",
    "        flat = 0\n",
    "        for i, s, st in zip(idx, self.shape, self.strides):\n",
    "            if i < 0 or i >= s:\n",
    "                raise IndexError(\"Index out of bounds\")\n",
    "            flat += i * st\n",
    "        return flat\n",
    "\n",
    "    def __getitem__(self, idx: Sequence[int]) -> float:\n",
    "        return self.data[self._index(idx)]\n",
    "\n",
    "    def __setitem__(self, idx: Sequence[int], value: Number) -> None:\n",
    "        self.data[self._index(idx)] = float(value)\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        if self.requires_grad:\n",
    "            self.grad = [0.0] * self.size\n",
    "\n",
    "    def _accum_grad(self, grad_data: List[float]) -> None:\n",
    "        if not self.requires_grad:\n",
    "            return\n",
    "        if self.grad is None:\n",
    "            self.grad = [0.0] * self.size\n",
    "        if len(grad_data) != self.size:\n",
    "            raise ValueError(\"Gradient size mismatch\")\n",
    "        for i in range(self.size):\n",
    "            self.grad[i] += grad_data[i]\n",
    "\n",
    "    def backward(self, grad: Optional[Union[Number, \"Tensor\"]] = None) -> None:\n",
    "        if not self.requires_grad:\n",
    "            raise RuntimeError(\"backward() called on tensor that does not require gradients\")\n",
    "        if grad is None:\n",
    "            if self.size != 1:\n",
    "                raise RuntimeError(\"grad must be specified for non-scalar outputs\")\n",
    "            grad_data = [1.0]\n",
    "        elif isinstance(grad, Tensor):\n",
    "            if grad.size != self.size:\n",
    "                raise ValueError(\"grad tensor must match output size\")\n",
    "            grad_data = grad.data[:]\n",
    "        else:\n",
    "            if self.size != 1:\n",
    "                raise RuntimeError(\"numeric grad only supported for scalar outputs\")\n",
    "            grad_data = [float(grad)]\n",
    "\n",
    "        topo: List[Tensor] = []\n",
    "        visited = set()\n",
    "\n",
    "        def build(v: Tensor):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build(self)\n",
    "        self._accum_grad(grad_data)\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def detach(self) -> \"Tensor\":\n",
    "        return Tensor(self.data[:], self.shape, requires_grad=False)\n",
    "\n",
    "    def reshape(self, *new_shape: int) -> \"Tensor\":\n",
    "        if -1 in new_shape:\n",
    "            raise ValueError(\"-1 reshape not supported\")\n",
    "        if math.prod(new_shape) != self.size:\n",
    "            raise ValueError(\"New shape size mismatch\")\n",
    "        out = Tensor(self.data[:], new_shape, requires_grad=self.requires_grad, _children=(self,), _op=\"reshape\")\n",
    "        def _backward():\n",
    "            if self.requires_grad and out.grad is not None:\n",
    "                self._accum_grad(out.grad[:])\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def transpose(self) -> \"Tensor\":\n",
    "        if len(self.shape) != 2:\n",
    "            raise ValueError(\"transpose expects 2D tensor\")\n",
    "        m, n = self.shape\n",
    "        out = Tensor.zeros((n, m), requires_grad=self.requires_grad)\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                out[j, i] = self[i, j]\n",
    "        out._prev = {self}\n",
    "        out._op = \"transpose\"\n",
    "        def _backward():\n",
    "            if self.requires_grad and out.grad is not None:\n",
    "                grad_self = [0.0] * self.size\n",
    "                # out shape (n,m), self shape (m,n)\n",
    "                for i in range(m):\n",
    "                    for j in range(n):\n",
    "                        grad_self[i * self.strides[0] + j] += out.grad[j * out.strides[0] + i]\n",
    "                self._accum_grad(grad_self)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    @property\n",
    "    def T(self) -> \"Tensor\":\n",
    "        return self.transpose()\n",
    "\n",
    "    def sum(self) -> \"Tensor\":\n",
    "        out = Tensor([sum(self.data)], (), requires_grad=self.requires_grad, _children=(self,), _op=\"sum\")\n",
    "        def _backward():\n",
    "            if self.requires_grad and out.grad is not None:\n",
    "                self._accum_grad([out.grad[0]] * self.size)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __neg__(self) -> \"Tensor\":\n",
    "        out = Tensor([-v for v in self.data], self.shape, requires_grad=self.requires_grad, _children=(self,), _op=\"neg\")\n",
    "        def _backward():\n",
    "            if self.requires_grad and out.grad is not None:\n",
    "                self._accum_grad([-g for g in out.grad])\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __add__(self, other: Union[\"Tensor\", Number]) -> \"Tensor\":\n",
    "        if isinstance(other, Tensor):\n",
    "            if self.shape != other.shape:\n",
    "                raise ValueError(\"Shape mismatch\")\n",
    "            out = Tensor([a + b for a, b in zip(self.data, other.data)], self.shape, requires_grad=(self.requires_grad or other.requires_grad), _children=(self, other), _op=\"+\")\n",
    "            def _backward():\n",
    "                if out.grad is None:\n",
    "                    return\n",
    "                if self.requires_grad:\n",
    "                    self._accum_grad(out.grad)\n",
    "                if other.requires_grad:\n",
    "                    other._accum_grad(out.grad)\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "        else:\n",
    "            c = float(other)\n",
    "            out = Tensor([a + c for a in self.data], self.shape, requires_grad=self.requires_grad, _children=(self,), _op=\"+c\")\n",
    "            def _backward():\n",
    "                if self.requires_grad and out.grad is not None:\n",
    "                    self._accum_grad(out.grad)\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "\n",
    "    def __radd__(self, other: Number) -> \"Tensor\":\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __sub__(self, other: Union[\"Tensor\", Number]) -> \"Tensor\":\n",
    "        if isinstance(other, Tensor):\n",
    "            if self.shape != other.shape:\n",
    "                raise ValueError(\"Shape mismatch\")\n",
    "            out = Tensor([a - b for a, b in zip(self.data, other.data)], self.shape, requires_grad=(self.requires_grad or other.requires_grad), _children=(self, other), _op=\"-\")\n",
    "            def _backward():\n",
    "                if out.grad is None:\n",
    "                    return\n",
    "                if self.requires_grad:\n",
    "                    self._accum_grad(out.grad)\n",
    "                if other.requires_grad:\n",
    "                    other._accum_grad([-g for g in out.grad])\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "        else:\n",
    "            c = float(other)\n",
    "            out = Tensor([a - c for a in self.data], self.shape, requires_grad=self.requires_grad, _children=(self,), _op=\"-c\")\n",
    "            def _backward():\n",
    "                if self.requires_grad and out.grad is not None:\n",
    "                    self._accum_grad(out.grad)\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "\n",
    "    def __rsub__(self, other: Number) -> \"Tensor\":\n",
    "        c = float(other)\n",
    "        return Tensor([c - a for a in self.data], self.shape, requires_grad=self.requires_grad, _children=(self,), _op=\"c-\")\n",
    "\n",
    "    def __mul__(self, other: Union[\"Tensor\", Number]) -> \"Tensor\":\n",
    "        if isinstance(other, Tensor):\n",
    "            if self.shape != other.shape:\n",
    "                raise ValueError(\"Shape mismatch\")\n",
    "            out = Tensor([a * b for a, b in zip(self.data, other.data)], self.shape, requires_grad=(self.requires_grad or other.requires_grad), _children=(self, other), _op=\"*\")\n",
    "            def _backward():\n",
    "                if out.grad is None:\n",
    "                    return\n",
    "                if self.requires_grad:\n",
    "                    self._accum_grad([b * g for b, g in zip(other.data, out.grad)])\n",
    "                if other.requires_grad:\n",
    "                    other._accum_grad([a * g for a, g in zip(self.data, out.grad)])\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "        else:\n",
    "            c = float(other)\n",
    "            out = Tensor([a * c for a in self.data], self.shape, requires_grad=self.requires_grad, _children=(self,), _op=\"*c\")\n",
    "            def _backward():\n",
    "                if self.requires_grad and out.grad is not None:\n",
    "                    self._accum_grad([c * g for g in out.grad])\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "\n",
    "    def __rmul__(self, other: Number) -> \"Tensor\":\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    def __truediv__(self, other: Union[\"Tensor\", Number]) -> \"Tensor\":\n",
    "        if isinstance(other, Tensor):\n",
    "            if self.shape != other.shape:\n",
    "                raise ValueError(\"Shape mismatch\")\n",
    "            out = Tensor([a / b for a, b in zip(self.data, other.data)], self.shape, requires_grad=(self.requires_grad or other.requires_grad), _children=(self, other), _op=\"/\")\n",
    "            def _backward():\n",
    "                if out.grad is None:\n",
    "                    return\n",
    "                if self.requires_grad:\n",
    "                    self._accum_grad([(1.0 / b) * g for b, g in zip(other.data, out.grad)])\n",
    "                if other.requires_grad:\n",
    "                    other._accum_grad([(-a / (b * b)) * g for a, b, g in zip(self.data, other.data, out.grad)])\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "        else:\n",
    "            c = float(other)\n",
    "            out = Tensor([a / c for a in self.data], self.shape, requires_grad=self.requires_grad, _children=(self,), _op=\"/c\")\n",
    "            def _backward():\n",
    "                if self.requires_grad and out.grad is not None:\n",
    "                    self._accum_grad([(1.0 / c) * g for g in out.grad])\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "\n",
    "    def __pow__(self, power: Number) -> \"Tensor\":\n",
    "        p = float(power)\n",
    "        out = Tensor([a ** p for a in self.data], self.shape, requires_grad=self.requires_grad, _children=(self,), _op=\"pow\")\n",
    "        def _backward():\n",
    "            if self.requires_grad and out.grad is not None:\n",
    "                self._accum_grad([(p * (a ** (p - 1.0))) * g for a, g in zip(self.data, out.grad)])\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def matmul(self, other: \"Tensor\") -> \"Tensor\":\n",
    "        if len(self.shape) != 2 or len(other.shape) != 2:\n",
    "            raise ValueError(\"matmul expects 2D tensors\")\n",
    "        m, n = self.shape\n",
    "        n2, p = other.shape\n",
    "        if n != n2:\n",
    "            raise ValueError(\"Inner dimensions mismatch\")\n",
    "        out = Tensor.zeros((m, p), requires_grad=(self.requires_grad or other.requires_grad))\n",
    "        for i in range(m):\n",
    "            for k in range(n):\n",
    "                a = self[i, k]\n",
    "                base = i * out.strides[0]\n",
    "                bbase = k * other.strides[0]\n",
    "                for j in range(p):\n",
    "                    out.data[base + j] += a * other.data[bbase + j]\n",
    "        out._prev = {self, other}\n",
    "        out._op = \"matmul\"\n",
    "        def _backward():\n",
    "            if out.grad is None:\n",
    "                return\n",
    "            if self.requires_grad:\n",
    "                grad_a = [0.0] * self.size\n",
    "                # dA = dOut @ B^T\n",
    "                for i in range(m):\n",
    "                    for k in range(n):\n",
    "                        s = 0.0\n",
    "                        for j in range(p):\n",
    "                            s += out.grad[i * out.strides[0] + j] * other.data[k * other.strides[0] + j]\n",
    "                        grad_a[i * self.strides[0] + k] += s\n",
    "                self._accum_grad(grad_a)\n",
    "            if other.requires_grad:\n",
    "                grad_b = [0.0] * other.size\n",
    "                # dB = A^T @ dOut\n",
    "                for k in range(n):\n",
    "                    for j in range(p):\n",
    "                        s = 0.0\n",
    "                        for i in range(m):\n",
    "                            s += self.data[i * self.strides[0] + k] * out.grad[i * out.strides[0] + j]\n",
    "                        grad_b[k * other.strides[0] + j] += s\n",
    "                other._accum_grad(grad_b)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        name = f\", name={self._name}\" if self._name else \"\"\n",
    "        rg = \", requires_grad=True\" if self.requires_grad else \"\"\n",
    "        return f\"Tensor(shape={self.shape}{name}{rg})\"\n",
    "\n",
    "\n",
    "def relu(x: Tensor) -> Tensor:\n",
    "    out = Tensor([v if v > 0 else 0.0 for v in x.data], x.shape, requires_grad=x.requires_grad, _children=(x,), _op=\"relu\")\n",
    "    def _backward():\n",
    "        if x.requires_grad and out.grad is not None:\n",
    "            x._accum_grad([g if v > 0 else 0.0 for v, g in zip(x.data, out.grad)])\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "\n",
    "def conv2d(\n",
    "    x: Tensor,\n",
    "    w: Tensor,\n",
    "    b: Optional[Tensor] = None,\n",
    "    *,\n",
    "    stride: int = 1,\n",
    "    padding: int = 0,\n",
    ") -> Tensor:\n",
    "    \"\"\"NCHW input, OIHW weights.\n",
    "\n",
    "    x: (N, C_in, H, W)\n",
    "    w: (C_out, C_in, KH, KW)\n",
    "    b: (C_out,) optional\n",
    "    out: (N, C_out, H_out, W_out)\n",
    "    \"\"\"\n",
    "    if len(x.shape) != 4 or len(w.shape) != 4:\n",
    "        raise ValueError(\"conv2d expects x as (N,C,H,W) and w as (O,C,KH,KW)\")\n",
    "    N, C_in, H, W = x.shape\n",
    "    C_out, C_in2, KH, KW = w.shape\n",
    "    if C_in != C_in2:\n",
    "        raise ValueError(\"conv2d channel mismatch\")\n",
    "    if b is not None and b.shape != (C_out,):\n",
    "        raise ValueError(\"bias must have shape (C_out,)\")\n",
    "    if stride <= 0 or padding < 0:\n",
    "        raise ValueError(\"stride must be >0 and padding >=0\")\n",
    "\n",
    "    H_out = (H + 2 * padding - KH) // stride + 1\n",
    "    W_out = (W + 2 * padding - KW) // stride + 1\n",
    "    if H_out <= 0 or W_out <= 0:\n",
    "        raise ValueError(\"Output spatial size is non-positive; check padding/stride/kernel\")\n",
    "\n",
    "    requires_grad = x.requires_grad or w.requires_grad or (b.requires_grad if b is not None else False)\n",
    "    out = Tensor.zeros((N, C_out, H_out, W_out), requires_grad=requires_grad)\n",
    "    out._prev = {x, w} | ({b} if b is not None else set())\n",
    "    out._op = \"conv2d\"\n",
    "\n",
    "    xs0, xs1, xs2, xs3 = x.strides\n",
    "    ws0, ws1, ws2, ws3 = w.strides\n",
    "    os0, os1, os2, os3 = out.strides\n",
    "\n",
    "    # forward\n",
    "    for n in range(N):\n",
    "        for co in range(C_out):\n",
    "            bias_val = b.data[co] if b is not None else 0.0\n",
    "            for ho in range(H_out):\n",
    "                hi0 = ho * stride - padding\n",
    "                for wo in range(W_out):\n",
    "                    wi0 = wo * stride - padding\n",
    "                    acc = bias_val\n",
    "                    for ci in range(C_in):\n",
    "                        x_base = n * xs0 + ci * xs1\n",
    "                        w_base = co * ws0 + ci * ws1\n",
    "                        for kh in range(KH):\n",
    "                            hi = hi0 + kh\n",
    "                            if hi < 0 or hi >= H:\n",
    "                                continue\n",
    "                            x_row = x_base + hi * xs2\n",
    "                            w_row = w_base + kh * ws2\n",
    "                            for kw in range(KW):\n",
    "                                wi = wi0 + kw\n",
    "                                if wi < 0 or wi >= W:\n",
    "                                    continue\n",
    "                                acc += x.data[x_row + wi * xs3] * w.data[w_row + kw * ws3]\n",
    "                    out.data[n * os0 + co * os1 + ho * os2 + wo * os3] = acc\n",
    "\n",
    "    def _backward():\n",
    "        if out.grad is None:\n",
    "            return\n",
    "        # dx\n",
    "        if x.requires_grad:\n",
    "            dx = [0.0] * x.size\n",
    "            for n in range(N):\n",
    "                for co in range(C_out):\n",
    "                    for ho in range(H_out):\n",
    "                        hi0 = ho * stride - padding\n",
    "                        for wo in range(W_out):\n",
    "                            wi0 = wo * stride - padding\n",
    "                            go = out.grad[n * os0 + co * os1 + ho * os2 + wo * os3]\n",
    "                            if go == 0.0:\n",
    "                                continue\n",
    "                            for ci in range(C_in):\n",
    "                                x_base = n * xs0 + ci * xs1\n",
    "                                w_base = co * ws0 + ci * ws1\n",
    "                                for kh in range(KH):\n",
    "                                    hi = hi0 + kh\n",
    "                                    if hi < 0 or hi >= H:\n",
    "                                        continue\n",
    "                                    x_row = x_base + hi * xs2\n",
    "                                    w_row = w_base + kh * ws2\n",
    "                                    for kw in range(KW):\n",
    "                                        wi = wi0 + kw\n",
    "                                        if wi < 0 or wi >= W:\n",
    "                                            continue\n",
    "                                        dx[x_row + wi * xs3] += w.data[w_row + kw * ws3] * go\n",
    "            x._accum_grad(dx)\n",
    "        # dw\n",
    "        if w.requires_grad:\n",
    "            dw = [0.0] * w.size\n",
    "            for co in range(C_out):\n",
    "                for ci in range(C_in):\n",
    "                    for kh in range(KH):\n",
    "                        for kw in range(KW):\n",
    "                            acc = 0.0\n",
    "                            for n in range(N):\n",
    "                                for ho in range(H_out):\n",
    "                                    hi = ho * stride - padding + kh\n",
    "                                    if hi < 0 or hi >= H:\n",
    "                                        continue\n",
    "                                    for wo in range(W_out):\n",
    "                                        wi = wo * stride - padding + kw\n",
    "                                        if wi < 0 or wi >= W:\n",
    "                                            continue\n",
    "                                        go = out.grad[n * os0 + co * os1 + ho * os2 + wo * os3]\n",
    "                                        acc += x.data[n * xs0 + ci * xs1 + hi * xs2 + wi * xs3] * go\n",
    "                            dw[co * ws0 + ci * ws1 + kh * ws2 + kw * ws3] = acc\n",
    "            w._accum_grad(dw)\n",
    "        # db\n",
    "        if b is not None and b.requires_grad:\n",
    "            db = [0.0] * b.size\n",
    "            for co in range(C_out):\n",
    "                s = 0.0\n",
    "                for n in range(N):\n",
    "                    base = n * os0 + co * os1\n",
    "                    for ho in range(H_out):\n",
    "                        row = base + ho * os2\n",
    "                        for wo in range(W_out):\n",
    "                            s += out.grad[row + wo * os3]\n",
    "                db[co] = s\n",
    "            b._accum_grad(db)\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "\n",
    "def maxpool2d(x: Tensor, *, kernel: int = 2, stride: Optional[int] = None) -> Tensor:\n",
    "    \"\"\"NCHW maxpool with square kernel.\"\"\"\n",
    "    if len(x.shape) != 4:\n",
    "        raise ValueError(\"maxpool2d expects x as (N,C,H,W)\")\n",
    "    if stride is None:\n",
    "        stride = kernel\n",
    "    if kernel <= 0 or stride <= 0:\n",
    "        raise ValueError(\"kernel/stride must be > 0\")\n",
    "    N, C, H, W = x.shape\n",
    "    H_out = (H - kernel) // stride + 1\n",
    "    W_out = (W - kernel) // stride + 1\n",
    "    if H_out <= 0 or W_out <= 0:\n",
    "        raise ValueError(\"Output spatial size is non-positive\")\n",
    "\n",
    "    out = Tensor.zeros((N, C, H_out, W_out), requires_grad=x.requires_grad)\n",
    "    out._prev = {x}\n",
    "    out._op = \"maxpool2d\"\n",
    "\n",
    "    xs0, xs1, xs2, xs3 = x.strides\n",
    "    os0, os1, os2, os3 = out.strides\n",
    "    argmax: List[int] = [0] * out.size\n",
    "    idx = 0\n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            for ho in range(H_out):\n",
    "                hi0 = ho * stride\n",
    "                for wo in range(W_out):\n",
    "                    wi0 = wo * stride\n",
    "                    best_val = -float(\"inf\")\n",
    "                    best_flat = 0\n",
    "                    for kh in range(kernel):\n",
    "                        hi = hi0 + kh\n",
    "                        for kw in range(kernel):\n",
    "                            wi = wi0 + kw\n",
    "                            flat = n * xs0 + c * xs1 + hi * xs2 + wi * xs3\n",
    "                            v = x.data[flat]\n",
    "                            if v > best_val:\n",
    "                                best_val = v\n",
    "                                best_flat = flat\n",
    "                    out.data[n * os0 + c * os1 + ho * os2 + wo * os3] = best_val\n",
    "                    argmax[idx] = best_flat\n",
    "                    idx += 1\n",
    "\n",
    "    def _backward():\n",
    "        if x.requires_grad and out.grad is not None:\n",
    "            dx = [0.0] * x.size\n",
    "            for i, g in enumerate(out.grad):\n",
    "                dx[argmax[i]] += g\n",
    "            x._accum_grad(dx)\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "\n",
    "def mse_loss(pred: Tensor, target: Tensor) -> Tensor:\n",
    "    if pred.shape != target.shape:\n",
    "        raise ValueError(\"mse_loss shape mismatch\")\n",
    "    diff = pred - target\n",
    "    return (diff * diff).sum() * (1.0 / pred.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e797d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV image -> custom Tensor\n",
    "from __future__ import annotations\n",
    "\n",
    "import cv2\n",
    "\n",
    "from typing import Literal, Optional, Tuple\n",
    "\n",
    "\n",
    "def image_to_tensor(\n",
    "    path: str,\n",
    "    *,\n",
    "    mode: Literal[\"color\", \"grayscale\", \"unchanged\"] = \"color\",\n",
    "    channel_order: Literal[\"bgr\", \"rgb\"] = \"rgb\",\n",
    "    normalize: bool = True,\n",
    "    resize_to: Optional[Tuple[int, int]] = None,  # (width, height)\n",
    "    add_batch_dim: bool = False,\n",
    "    dtype: Literal[\"float\", \"int\"] = \"float\",\n",
    ") -> Tensor:\n",
    "    \"\"\"Load an image from disk with OpenCV and convert to your Tensor.\n",
    "\n",
    "    - unchanged: whatever OpenCV reads (can be (H,W), (H,W,3), (H,W,4))\n",
    "    If add_batch_dim=True, shape becomes (1, ...)\n",
    "    \"\"\"\n",
    "    if not path or not isinstance(path, str):\n",
    "        raise ValueError(\"path must be a non-empty string\")\n",
    "\n",
    "    if mode == \"color\":\n",
    "        flag = cv2.IMREAD_COLOR\n",
    "    elif mode == \"grayscale\":\n",
    "        flag = cv2.IMREAD_GRAYSCALE\n",
    "    elif mode == \"unchanged\":\n",
    "        flag = cv2.IMREAD_UNCHANGED\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "    img = cv2.imread(path, flag)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Could not read image at: {path}\")\n",
    "\n",
    "    if resize_to is not None:\n",
    "        w, h = resize_to\n",
    "        img = cv2.resize(img, (int(w), int(h)), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Convert BGR->RGB if requested and image has channels\n",
    "    if channel_order == \"rgb\" and mode != \"grayscale\" and getattr(img, \"ndim\", 0) == 3:\n",
    "        # Only swap first 3 channels; preserves alpha if present (BGRA->RGBA)\n",
    "        if img.shape[2] == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        elif img.shape[2] == 4:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGRA2RGBA)\n",
    "    elif channel_order != \"bgr\" and channel_order != \"rgb\":\n",
    "        raise ValueError(f\"Unknown channel_order: {channel_order}\")\n",
    "\n",
    "    flat = img.reshape(-1).tolist()\n",
    "    if dtype == \"float\":\n",
    "        if normalize:\n",
    "            flat = [v / 255.0 for v in flat]\n",
    "        else:\n",
    "            flat = [float(v) for v in flat]\n",
    "    elif dtype == \"int\":\n",
    "        # keep ints (and ignore normalize)\n",
    "        flat = [int(v) for v in flat]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dtype: {dtype}\")\n",
    "\n",
    "    shape = tuple(int(s) for s in img.shape)\n",
    "    if add_batch_dim:\n",
    "        shape = (1,) + shape\n",
    "    return Tensor(flat, shape)\n",
    "\n",
    "\n",
    "# image_path = \"/path/to/your/image.jpg\"\n",
    "# img_tensor = image_to_tensor(image_path, mode=\"color\", channel_order=\"rgb\", normalize=True)\n",
    "# print(\"tensor shape:\", img_tensor.shape)\n",
    "# print(\"first 10 values:\", img_tensor.data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb030e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 17.401077974429356\n",
      "x.grad nonzero? True\n",
      "w.grad nonzero? True\n",
      "b.grad: [1.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: conv2d -> relu -> loss -> backward\n",
    "x = Tensor.randn((1, 1, 5, 5), requires_grad=True, seed=0).named(\"x\")\n",
    "w = Tensor.randn((2, 1, 3, 3), requires_grad=True, seed=1).named(\"w\")\n",
    "b = Tensor.zeros((2,), requires_grad=True).named(\"b\")\n",
    "\n",
    "y = conv2d(x, w, b, stride=1, padding=0)\n",
    "y = relu(y)\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(\"loss:\", loss.item())\n",
    "print(\"x.grad nonzero?\", any(abs(g) > 1e-12 for g in (x.grad or [])))\n",
    "print(\"w.grad nonzero?\", any(abs(g) > 1e-12 for g in (w.grad or [])))\n",
    "print(\"b.grad:\", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74284f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.3032716773973307\n",
      "probs[0] sum: 0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "# Layers / model / loss / optimizers for 32x32 image classification (NCHW)\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import random\n",
    "from typing import Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utilities\n",
    "# =========================\n",
    "def as_nchw(img: Tensor, *, add_batch: bool = True) -> Tensor:\n",
    "    \"\"\"Convert (H,W,C) or (H,W) to (N,C,H,W).\"\"\"\n",
    "    if len(img.shape) == 2:\n",
    "        H, W = img.shape\n",
    "        C = 1\n",
    "        chw: List[float] = img.data[:]\n",
    "        out = Tensor(chw, (C, H, W), requires_grad=img.requires_grad, _children=(img,), _op=\"as_nchw\")\n",
    "        def _backward():\n",
    "            if img.requires_grad and out.grad is not None:\n",
    "                img._accum_grad(out.grad[:])\n",
    "        out._backward = _backward\n",
    "    elif len(img.shape) == 3:\n",
    "        H, W, C = img.shape\n",
    "        if C not in (1, 3, 4):\n",
    "            raise ValueError(\"Expected C in {1,3,4}\")\n",
    "        # HWC -> CHW\n",
    "        chw = [0.0] * (C * H * W)\n",
    "        for h in range(H):\n",
    "            for w_ in range(W):\n",
    "                base = (h * W + w_) * C\n",
    "                for c in range(C):\n",
    "                    chw[c * H * W + h * W + w_] = img.data[base + c]\n",
    "        out = Tensor(chw, (C, H, W), requires_grad=img.requires_grad, _children=(img,), _op=\"as_nchw\")\n",
    "        def _backward():\n",
    "            if img.requires_grad and out.grad is not None:\n",
    "                grad_img = [0.0] * img.size\n",
    "                for h in range(H):\n",
    "                    for w_ in range(W):\n",
    "                        base = (h * W + w_) * C\n",
    "                        for c in range(C):\n",
    "                            grad_img[base + c] += out.grad[c * H * W + h * W + w_]\n",
    "                img._accum_grad(grad_img)\n",
    "        out._backward = _backward\n",
    "    else:\n",
    "        raise ValueError(\"as_nchw expects (H,W) or (H,W,C)\")\n",
    "\n",
    "    if add_batch:\n",
    "        out2 = Tensor(out.data[:], (1,) + out.shape, requires_grad=out.requires_grad, _children=(out,), _op=\"add_batch\")\n",
    "        def _backward2():\n",
    "            if out.requires_grad and out2.grad is not None:\n",
    "                out._accum_grad(out2.grad[:])\n",
    "        out2._backward = _backward2\n",
    "        return out2\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Core module system\n",
    "# =========================\n",
    "class Module:\n",
    "    def parameters(self) -> List[Tensor]:\n",
    "        params: List[Tensor] = []\n",
    "        for v in self.__dict__.values():\n",
    "            if isinstance(v, Tensor) and v.requires_grad:\n",
    "                params.append(v)\n",
    "            elif isinstance(v, Module):\n",
    "                params.extend(v.parameters())\n",
    "            elif isinstance(v, (list, tuple)):\n",
    "                for item in v:\n",
    "                    if isinstance(item, Tensor) and item.requires_grad:\n",
    "                        params.append(item)\n",
    "                    elif isinstance(item, Module):\n",
    "                        params.extend(item.parameters())\n",
    "        return params\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        for p in self.parameters():\n",
    "            p.zero_grad()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Activation functions\n",
    "# =========================\n",
    "def softmax(x: Tensor) -> Tensor:\n",
    "    \"\"\"Softmax over classes for logits shaped (N, C).\"\"\"\n",
    "    if len(x.shape) != 2:\n",
    "        raise ValueError(\"softmax expects (N,C)\")\n",
    "    N, C = x.shape\n",
    "    probs: List[float] = [0.0] * x.size\n",
    "    # forward stable softmax row-wise\n",
    "    for n in range(N):\n",
    "        row = [x.data[n * x.strides[0] + c * x.strides[1]] for c in range(C)]\n",
    "        m = max(row)\n",
    "        exps = [math.exp(v - m) for v in row]\n",
    "        s = sum(exps)\n",
    "        for c in range(C):\n",
    "            probs[n * x.strides[0] + c * x.strides[1]] = exps[c] / s\n",
    "\n",
    "    out = Tensor(probs, x.shape, requires_grad=x.requires_grad, _children=(x,), _op=\"softmax\")\n",
    "    def _backward():\n",
    "        if not x.requires_grad or out.grad is None:\n",
    "            return\n",
    "        dx = [0.0] * x.size\n",
    "        # Jacobian-vector product per row: p * (g - sum(g*p))\n",
    "        for n in range(N):\n",
    "            dot = 0.0\n",
    "            for c in range(C):\n",
    "                p = out.data[n * out.strides[0] + c * out.strides[1]]\n",
    "                g = out.grad[n * out.strides[0] + c * out.strides[1]]\n",
    "                dot += g * p\n",
    "            for c in range(C):\n",
    "                p = out.data[n * out.strides[0] + c * out.strides[1]]\n",
    "                g = out.grad[n * out.strides[0] + c * out.strides[1]]\n",
    "                dx[n * x.strides[0] + c * x.strides[1]] += p * (g - dot)\n",
    "        x._accum_grad(dx)\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "\n",
    "class ReLU(Module):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return relu(x)\n",
    "\n",
    "\n",
    "class Softmax(Module):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return softmax(x)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Pooling layers\n",
    "# =========================\n",
    "class MaxPool2D(Module):\n",
    "    def __init__(self, kernel: int = 2, stride: Optional[int] = None):\n",
    "        self.kernel = int(kernel)\n",
    "        self.stride = None if stride is None else int(stride)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return maxpool2d(x, kernel=self.kernel, stride=self.stride)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Learnable layers\n",
    "# =========================\n",
    "class Flatten(Module):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if len(x.shape) != 4:\n",
    "            raise ValueError(\"Flatten expects NCHW\")\n",
    "        N, C, H, W = x.shape\n",
    "        return x.reshape(N, C * H * W)\n",
    "\n",
    "\n",
    "class Conv2D(Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, *, stride: int = 1, padding: int = 0):\n",
    "        self.in_channels = int(in_channels)\n",
    "        self.out_channels = int(out_channels)\n",
    "        self.kernel_size = int(kernel_size)\n",
    "        self.stride = int(stride)\n",
    "        self.padding = int(padding)\n",
    "        # Kaiming-like init (simple)\n",
    "        fan_in = self.in_channels * self.kernel_size * self.kernel_size\n",
    "        scale = math.sqrt(2.0 / fan_in)\n",
    "        self.weight = Tensor.randn((self.out_channels, self.in_channels, self.kernel_size, self.kernel_size), std=scale, requires_grad=True)\n",
    "        self.bias = Tensor.zeros((self.out_channels,), requires_grad=True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return conv2d(x, self.weight, self.bias, stride=self.stride, padding=self.padding)\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        self.in_features = int(in_features)\n",
    "        self.out_features = int(out_features)\n",
    "        scale = math.sqrt(2.0 / self.in_features)\n",
    "        self.weight = Tensor.randn((self.out_features, self.in_features), std=scale, requires_grad=True)  # (out,in)\n",
    "        self.bias = Tensor.zeros((self.out_features,), requires_grad=True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if len(x.shape) != 2:\n",
    "            raise ValueError(\"Linear expects (N, in_features)\")\n",
    "        N, D = x.shape\n",
    "        if D != self.in_features:\n",
    "            raise ValueError(\"Linear input feature mismatch\")\n",
    "        W = self.weight\n",
    "        b = self.bias\n",
    "        out = Tensor.zeros((N, self.out_features), requires_grad=(x.requires_grad or W.requires_grad or b.requires_grad))\n",
    "        out._prev = {x, W, b}\n",
    "        out._op = \"linear\"\n",
    "        xs0, xs1 = x.strides\n",
    "        ws0, ws1 = W.strides\n",
    "        os0, os1 = out.strides\n",
    "        # forward: y = x @ W^T + b\n",
    "        for n in range(N):\n",
    "            for o in range(self.out_features):\n",
    "                acc = b.data[o]\n",
    "                wrow = o * ws0\n",
    "                xrow = n * xs0\n",
    "                for d in range(D):\n",
    "                    acc += x.data[xrow + d * xs1] * W.data[wrow + d * ws1]\n",
    "                out.data[n * os0 + o * os1] = acc\n",
    "\n",
    "        def _backward():\n",
    "            if out.grad is None:\n",
    "                return\n",
    "            if x.requires_grad:\n",
    "                dx = [0.0] * x.size\n",
    "                for n in range(N):\n",
    "                    xrow = n * xs0\n",
    "                    for d in range(D):\n",
    "                        s = 0.0\n",
    "                        for o in range(self.out_features):\n",
    "                            s += out.grad[n * os0 + o * os1] * W.data[o * ws0 + d * ws1]\n",
    "                        dx[xrow + d * xs1] += s\n",
    "                x._accum_grad(dx)\n",
    "            if W.requires_grad:\n",
    "                dW = [0.0] * W.size\n",
    "                for o in range(self.out_features):\n",
    "                    wrow = o * ws0\n",
    "                    for d in range(D):\n",
    "                        s = 0.0\n",
    "                        for n in range(N):\n",
    "                            s += x.data[n * xs0 + d * xs1] * out.grad[n * os0 + o * os1]\n",
    "                        dW[wrow + d * ws1] += s\n",
    "                W._accum_grad(dW)\n",
    "            if b.requires_grad:\n",
    "                db = [0.0] * b.size\n",
    "                for o in range(self.out_features):\n",
    "                    s = 0.0\n",
    "                    for n in range(N):\n",
    "                        s += out.grad[n * os0 + o * os1]\n",
    "                    db[o] += s\n",
    "                b._accum_grad(db)\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Loss functions\n",
    "# =========================\n",
    "def cross_entropy_loss(logits: Tensor, targets: Sequence[int]) -> Tensor:\n",
    "    \"\"\"Softmax cross-entropy. logits: (N,C), targets: length N ints in [0,C).\"\"\"\n",
    "    if len(logits.shape) != 2:\n",
    "        raise ValueError(\"cross_entropy_loss expects logits (N,C)\")\n",
    "    N, C = logits.shape\n",
    "    if len(targets) != N:\n",
    "        raise ValueError(\"targets length must equal N\")\n",
    "    for t in targets:\n",
    "        if not (0 <= int(t) < C):\n",
    "            raise ValueError(\"target out of range\")\n",
    "\n",
    "    probs = [[0.0] * C for _ in range(N)]\n",
    "    loss_val = 0.0\n",
    "    for n in range(N):\n",
    "        row = [logits.data[n * logits.strides[0] + c * logits.strides[1]] for c in range(C)]\n",
    "        m = max(row)\n",
    "        exps = [math.exp(v - m) for v in row]\n",
    "        s = sum(exps)\n",
    "        p = [e / s for e in exps]\n",
    "        probs[n] = p\n",
    "        loss_val += -math.log(max(p[int(targets[n])], 1e-12))\n",
    "    loss_val /= N\n",
    "\n",
    "    out = Tensor([loss_val], (), requires_grad=logits.requires_grad, _children=(logits,), _op=\"cross_entropy\")\n",
    "    def _backward():\n",
    "        if not logits.requires_grad or out.grad is None:\n",
    "            return\n",
    "        g = out.grad[0]\n",
    "        dlogits = [0.0] * logits.size\n",
    "        for n in range(N):\n",
    "            t = int(targets[n])\n",
    "            for c in range(C):\n",
    "                grad = probs[n][c]\n",
    "                if c == t:\n",
    "                    grad -= 1.0\n",
    "                dlogits[n * logits.strides[0] + c * logits.strides[1]] += (grad / N) * g\n",
    "        logits._accum_grad(dlogits)\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Optimizers\n",
    "# =========================\n",
    "class Optimizer:\n",
    "    def __init__(self, params: Iterable[Tensor]):\n",
    "        self.params = [p for p in params]\n",
    "    def step(self) -> None:\n",
    "        raise NotImplementedError\n",
    "    def zero_grad(self) -> None:\n",
    "        for p in self.params:\n",
    "            p.zero_grad()\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, params: Iterable[Tensor], *, lr: float = 1e-2, momentum: float = 0.0, weight_decay: float = 0.0):\n",
    "        super().__init__(params)\n",
    "        self.lr = float(lr)\n",
    "        self.momentum = float(momentum)\n",
    "        self.weight_decay = float(weight_decay)\n",
    "        self._v: dict[int, List[float]] = {}\n",
    "    def step(self) -> None:\n",
    "        for p in self.params:\n",
    "            if not p.requires_grad or p.grad is None:\n",
    "                continue\n",
    "            grad = p.grad[:]\n",
    "            if self.weight_decay != 0.0:\n",
    "                for i in range(p.size):\n",
    "                    grad[i] += self.weight_decay * p.data[i]\n",
    "            if self.momentum != 0.0:\n",
    "                vid = id(p)\n",
    "                v = self._v.get(vid)\n",
    "                if v is None:\n",
    "                    v = [0.0] * p.size\n",
    "                    self._v[vid] = v\n",
    "                for i in range(p.size):\n",
    "                    v[i] = self.momentum * v[i] + grad[i]\n",
    "                    p.data[i] -= self.lr * v[i]\n",
    "            else:\n",
    "                for i in range(p.size):\n",
    "                    p.data[i] -= self.lr * grad[i]\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, params: Iterable[Tensor], *, lr: float = 1e-3, betas: Tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.0):\n",
    "        super().__init__(params)\n",
    "        self.lr = float(lr)\n",
    "        self.beta1, self.beta2 = float(betas[0]), float(betas[1])\n",
    "        self.eps = float(eps)\n",
    "        self.weight_decay = float(weight_decay)\n",
    "        self.t = 0\n",
    "        self.m: dict[int, List[float]] = {}\n",
    "        self.v: dict[int, List[float]] = {}\n",
    "    def step(self) -> None:\n",
    "        self.t += 1\n",
    "        b1, b2 = self.beta1, self.beta2\n",
    "        for p in self.params:\n",
    "            if not p.requires_grad or p.grad is None:\n",
    "                continue\n",
    "            g = p.grad[:]\n",
    "            if self.weight_decay != 0.0:\n",
    "                for i in range(p.size):\n",
    "                    g[i] += self.weight_decay * p.data[i]\n",
    "            pid = id(p)\n",
    "            m = self.m.get(pid)\n",
    "            v = self.v.get(pid)\n",
    "            if m is None:\n",
    "                m = [0.0] * p.size\n",
    "                v = [0.0] * p.size\n",
    "                self.m[pid] = m\n",
    "                self.v[pid] = v\n",
    "            for i in range(p.size):\n",
    "                m[i] = b1 * m[i] + (1.0 - b1) * g[i]\n",
    "                v[i] = b2 * v[i] + (1.0 - b2) * (g[i] * g[i])\n",
    "                mhat = m[i] / (1.0 - (b1 ** self.t))\n",
    "                vhat = v[i] / (1.0 - (b2 ** self.t))\n",
    "                p.data[i] -= self.lr * mhat / (math.sqrt(vhat) + self.eps)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Models\n",
    "# =========================\n",
    "class SimpleCNN(Module):\n",
    "    \"\"\"Example model for 32x32 images. Output logits (N,num_classes).\"\"\"\n",
    "    def __init__(self, in_channels: int = 3, num_classes: int = 10):\n",
    "        self.conv1 = Conv2D(in_channels, 8, 3, padding=1)   # 32x32\n",
    "        self.act1 = ReLU()\n",
    "        self.pool1 = MaxPool2D(2)                           # 16x16\n",
    "        self.conv2 = Conv2D(8, 16, 3, padding=1)            # 16x16\n",
    "        self.act2 = ReLU()\n",
    "        self.pool2 = MaxPool2D(2)                           # 8x8\n",
    "        self.flat = Flatten()\n",
    "        self.fc = Linear(16 * 8 * 8, num_classes)\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.pool1(self.act1(self.conv1(x)))\n",
    "        x = self.pool2(self.act2(self.conv2(x)))\n",
    "        x = self.flat(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Quick smoke test: one training step on random 32x32 batch\n",
    "model = SimpleCNN(in_channels=3, num_classes=10)\n",
    "optim = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "N = 4\n",
    "x_batch = Tensor.randn((N, 3, 32, 32), requires_grad=False, seed=42)\n",
    "y_batch = [0, 1, 2, 3]  # fake labels\n",
    "logits = model(x_batch)\n",
    "loss = cross_entropy_loss(logits, y_batch)\n",
    "optim.zero_grad()\n",
    "loss.backward()\n",
    "optim.step()\n",
    "print(\"loss:\", loss.item())\n",
    "\n",
    "# (Optional) probabilities for inspection\n",
    "probs = Softmax()(logits.detach())\n",
    "print(\"probs[0] sum:\", sum(probs.data[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c84c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(loss): 6.664145446092037\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from typing import List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "\n",
    "def _ensure_nchw(x: Tensor) -> Tensor:\n",
    "    \"\"\"Accepts (H,W,C), (N,H,W,C), (N,C,H,W) and returns (N,C,H,W).\"\"\"\n",
    "    if len(x.shape) == 3:\n",
    "        # HWC -> NCHW\n",
    "        return as_nchw(x, add_batch=True)\n",
    "    if len(x.shape) == 4:\n",
    "        # Could be NCHW or NHWC. Heuristic: if second dim is 1/3/4 treat as NCHW, else treat as NHWC.\n",
    "        N, A, B, C = x.shape\n",
    "        if A in (1, 3, 4):\n",
    "            return x\n",
    "        # NHWC -> NCHW\n",
    "        H, W, Ch = A, B, C\n",
    "        nchw = [0.0] * (N * Ch * H * W)\n",
    "        for n in range(N):\n",
    "            for h in range(H):\n",
    "                for w_ in range(W):\n",
    "                    base = n * (H * W * Ch) + (h * W + w_) * Ch\n",
    "                    for c in range(Ch):\n",
    "                        nchw[n * (Ch * H * W) + c * (H * W) + h * W + w_] = x.data[base + c]\n",
    "        out = Tensor(nchw, (N, Ch, H, W), requires_grad=x.requires_grad, _children=(x,), _op=\"nhwc_to_nchw\")\n",
    "        def _backward():\n",
    "            if x.requires_grad and out.grad is not None:\n",
    "                grad_x = [0.0] * x.size\n",
    "                for n in range(N):\n",
    "                    for h in range(H):\n",
    "                        for w_ in range(W):\n",
    "                            base = n * (H * W * Ch) + (h * W + w_) * Ch\n",
    "                            for c in range(Ch):\n",
    "                                grad_x[base + c] += out.grad[n * (Ch * H * W) + c * (H * W) + h * W + w_]\n",
    "                x._accum_grad(grad_x)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    raise ValueError(\"Expected input as (H,W,C), (N,H,W,C), or (N,C,H,W)\")\n",
    "\n",
    "\n",
    "def _apply_activation(x: Tensor, activation: Optional[str]) -> Tensor:\n",
    "    if activation is None:\n",
    "        return x\n",
    "    act = str(activation).lower()\n",
    "    if act == \"relu\":\n",
    "        return relu(x)\n",
    "    if act == \"softmax\":\n",
    "        return softmax(x)\n",
    "    raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "\n",
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self):\n",
    "        self.layers: List[Module] = []\n",
    "    def add(self, layer: Module) -> None:\n",
    "        self.layers.append(layer)\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class _Conv2D_(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filters: int,\n",
    "        kernel_size: Union[int, Tuple[int, int]] = (3, 3),\n",
    "        *,\n",
    "        activation: Optional[str] = None,\n",
    "        input_shape: Optional[Tuple[int, int, int]] = None,  # (H,W,C) channels_last\n",
    "        stride: int = 1,\n",
    "        padding: Union[int, str] = \"valid\",\n",
    "    ):\n",
    "        self.filters = int(filters)\n",
    "        if isinstance(kernel_size, int):\n",
    "            self.kh = self.kw = int(kernel_size)\n",
    "        else:\n",
    "            self.kh = int(kernel_size[0])\n",
    "            self.kw = int(kernel_size[1])\n",
    "        self.activation = activation\n",
    "        self.input_shape = input_shape\n",
    "        self.stride = int(stride)\n",
    "        self.padding = padding\n",
    "        self.weight: Optional[Tensor] = None\n",
    "        self.bias: Optional[Tensor] = None\n",
    "        # If input_shape is provided, we can initialize immediately\n",
    "        if input_shape is not None:\n",
    "            in_ch = int(input_shape[2])\n",
    "            self._build(in_ch)\n",
    "\n",
    "    def _build(self, in_channels: int) -> None:\n",
    "        fan_in = in_channels * self.kh * self.kw\n",
    "        scale = math.sqrt(2.0 / fan_in)\n",
    "        self.weight = Tensor.randn((self.filters, in_channels, self.kh, self.kw), std=scale, requires_grad=True)\n",
    "        self.bias = Tensor.zeros((self.filters,), requires_grad=True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = _ensure_nchw(x)\n",
    "        in_ch = x.shape[1]\n",
    "        if self.weight is None or self.bias is None:\n",
    "            self._build(in_ch)\n",
    "        pad = 0\n",
    "        if isinstance(self.padding, str):\n",
    "            p = self.padding.lower()\n",
    "            if p == \"valid\":\n",
    "                pad = 0\n",
    "            elif p == \"same\":\n",
    "                if self.kh != self.kw:\n",
    "                    raise ValueError(\"padding='same' only supported for square kernels in this implementation\")\n",
    "                pad = self.kh // 2\n",
    "            else:\n",
    "                raise ValueError(\"padding must be 'valid', 'same', or an int\")\n",
    "        else:\n",
    "            pad = int(self.padding)\n",
    "        y = conv2d(x, self.weight, self.bias, stride=self.stride, padding=pad)\n",
    "        y = _apply_activation(y, self.activation)\n",
    "        return y\n",
    "\n",
    "\n",
    "class _MaxPooling2D_(Module):\n",
    "    def __init__(self, pool_size: Tuple[int, int] = (2, 2), *, strides: Optional[Tuple[int, int]] = None):\n",
    "        self.pool_size = (int(pool_size[0]), int(pool_size[1]))\n",
    "        self.strides = None if strides is None else (int(strides[0]), int(strides[1]))\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = _ensure_nchw(x)\n",
    "        kh, kw = self.pool_size\n",
    "        if kh != kw:\n",
    "            raise ValueError(\"Only square pool_size supported (e.g., (2,2))\")\n",
    "        if self.strides is None:\n",
    "            return maxpool2d(x, kernel=kh, stride=kh)\n",
    "        sh, sw = self.strides\n",
    "        if sh != sw:\n",
    "            raise ValueError(\"Only square strides supported (e.g., (2,2))\")\n",
    "        return maxpool2d(x, kernel=kh, stride=sh)\n",
    "\n",
    "\n",
    "class _Flatten_(Module):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if len(x.shape) == 4:\n",
    "            N, C, H, W = x.shape\n",
    "            return x.reshape(N, C * H * W)\n",
    "        if len(x.shape) == 2:\n",
    "            return x\n",
    "        raise ValueError(\"Flatten expects (N,C,H,W) or already-flat (N,D)\")\n",
    "\n",
    "\n",
    "class _Dense_(Module):\n",
    "    def __init__(self, units: int, *, activation: Optional[str] = None, input_shape: Optional[Tuple[int]] = None):\n",
    "        self.units = int(units)\n",
    "        self.activation = activation\n",
    "        self.input_shape = input_shape\n",
    "        self.linear: Optional[Linear] = None\n",
    "        if input_shape is not None:\n",
    "            self.linear = Linear(int(input_shape[0]), self.units)\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if len(x.shape) != 2:\n",
    "            raise ValueError(\"Dense expects (N,D). Add Flatten() before Dense for conv nets.\")\n",
    "        if self.linear is None:\n",
    "            self.linear = Linear(x.shape[1], self.units)\n",
    "        y = self.linear(x)\n",
    "        y = _apply_activation(y, self.activation)\n",
    "        return y\n",
    "    def parameters(self) -> List[Tensor]:\n",
    "        return [] if self.linear is None else self.linear.parameters()\n",
    "\n",
    "\n",
    "class layers:\n",
    "    Conv2D = _Conv2D_\n",
    "    MaxPooling2D = _MaxPooling2D_\n",
    "    Flatten = _Flatten_\n",
    "    Dense = _Dense_\n",
    "    ReLU = ReLU\n",
    "    Softmax = Softmax\n",
    "\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(10))\n",
    "\n",
    "x_nhwc = Tensor.randn((4, 32, 32, 3), seed=7) \n",
    "y_true = [0, 1, 2, 3]\n",
    "logits2 = model2(x_nhwc)\n",
    "loss2 = cross_entropy_loss(logits2, y_true)\n",
    "opt2 = Adam(model2.parameters(), lr=1e-3)\n",
    "opt2.zero_grad()\n",
    "loss2.backward()\n",
    "opt2.step()\n",
    "print(\"Sequential(loss):\", loss2.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
